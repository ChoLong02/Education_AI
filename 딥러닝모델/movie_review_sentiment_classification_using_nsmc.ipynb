{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChoLong02/Education_AI/blob/master/%EB%94%A5%EB%9F%AC%EB%8B%9D%EB%AA%A8%EB%8D%B8/movie_review_sentiment_classification_using_nsmc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습하기\n",
        "  - 영화 리뷰 긍정, 부정 분류기\n",
        "  - 딥러닝(Tensorflow+Keras)\n",
        "  - 데이터: NSMC(네이버 영화 리뷰 말뭉치)"
      ],
      "metadata": {
        "id": "gt8zTm72DcJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8b-K06K3bl-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c547390-f57b-4dcf-bccb-2d4edf4aa67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# Linux shell에서 실행!\n",
        "!pip install --target=$my_path konlpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23fcbYNmdcLk"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# f = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vqcw4YG8ahbU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from konlpy.tag import Okt\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B8Gcx0sXcf4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1e98ee-1c55-4726-ab6c-4ba870784d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150000 3\n",
            "50000 3\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 파일 열기 #\n",
        "#############\n",
        "\n",
        "# *.txt 파일에서 데이터를 로드하는 함수\n",
        "def read_data(filename):\n",
        "  with open(filename, 'r', encoding='UTF-8') as f:\n",
        "    # f → line 15만개\n",
        "    # line: 9976970\t아 더빙.. 진짜 짜증나네요 목소리\t0\n",
        "    # split() 분할, \\t: tab기준\n",
        "    # data = [9976970, 아 더빙.. 진짜 짜증나네요 목소리, 0]\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "    data = data[1:]  # 150001개 → 첫번째 제목열 제거(id, doc, label)\n",
        "  return data\n",
        "\n",
        "# nsmc 데이터 불러오기\n",
        "train_data = read_data('/content/drive/MyDrive/nsmc/ratings_train.txt')  # 매개변수 안에는 파일의 path(경로)\n",
        "test_data = read_data('/content/drive/MyDrive/nsmc/ratings_test.txt')\n",
        "\n",
        "# 데이터 확인\n",
        "print(len(train_data), len(train_data[0]))\n",
        "print(len(test_data), len(test_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "# 데이터 전처리 #\n",
        "#################\n",
        "# 데이터를 인공지능 학습에 맞게 전처리\n",
        "# ※ 인공지능은 학습데이터의 shape이 항상 동일해야함\n",
        "# 이미지(25x25), 이미지(45x45) → 학습불가\n",
        "# 전처리 2개 이미지의 사이즈를 동일(30x30) → 학습가능\n",
        "\n",
        "# 네이버 리뷰는 맞춤법, 띄어쓰기 등이 제래도 안되있음\n",
        "# 형태소분석, 품사태깅 등을 통해 전처리\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "_OFJuzTtEkhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요\")\n",
        "# print(okt.pos(\"이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요\"))"
      ],
      "metadata": {
        "id": "D9e3B460FX3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(doc):\n",
        "  # norm(정규화) : 그래욬ㅋㅋ → 그래요\n",
        "  # stem(근어 표시) : 그래요 → 그렇다(원형)\n",
        "  # 아 더빙.. 진짜 짜증나네요 목소리\n",
        "  # t = 아, 더빙, 진짜, 짜증, 나네요, 목소리\n",
        "  # join() → ['A', 'B', 'C'] >> ['A/B/C']\n",
        "  return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
        "\n",
        "# train_docs.json: 전처리 완료 된 파일\n",
        "# isfile(): ()안의 경로에 file이 존재하는지 여부\n",
        "# → txt, csv, json\n",
        "if os.path.isfile('/content/drive/MyDrive/nsmc/train_docs.json'):\n",
        "  with open('/content/drive/MyDrive/nsmc/train_docs.json', 'r', encoding='UTF-8') as f:\n",
        "    train_docs = json.load(f)\n",
        "  with open('/content/drive/MyDrive/nsmc/test_docs.json', 'r', encoding='UTF-8') as f:\n",
        "    test_docs = json.load(f)\n",
        "else:\n",
        "  # 없으면 전처리 진행!\n",
        "  # [] List → 복수 건의 값을 저장(자료형 상이해도 문제없음)\n",
        "  # train_data → 150000개, 3개(id, document, label)\n",
        "  # row → [id, document, label]\n",
        "  # [???, label]\n",
        "  train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
        "  test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
        "\n",
        "  with open('/content/drive/MyDrive/nsmc/train_docs.json', 'w', encoding='UTF-8') as make_file:\n",
        "    json.dump(train_docs, make_file, ensure_ascii=False, indent='\\t')\n",
        "  with open('/content/drive/MyDrive/nsmc/test_docs.json', 'w', encoding='UTF-8') as make_file:\n",
        "    json.dump(test_docs, make_file, ensure_ascii=False, indent='\\t')\n",
        "\n",
        "print(train_docs[0])\n",
        "print(test_docs[0])\n",
        "print(len(train_docs[0]))\n",
        "print(len(test_docs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-DTisgKF8It",
        "outputId": "f9d503f1-a6e3-4a2c-8393-a343688d90c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['아/Exclamation', '더빙/Noun', '../Punctuation', '진짜/Noun', '짜증나다/Adjective', '목소리/Noun'], '0']\n",
            "[['굳다/Adjective', 'ㅋ/KoreanParticle'], '1']\n",
            "2\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 train 데이터의 Token 수를 count\n",
        "# [[token, token], label]\n",
        "# d = [token, token], label\n",
        "# d[0] = [token, token, token, toekn, ... 15만개 리뷰 토큰들]\n",
        "tokens = [t for d in train_docs for t in d[0]]  # //[[token, token], label]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLH9BO41Qe2C",
        "outputId": "b83d151d-d719-48aa-ae00-b80221e7cce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2159921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "# 데이터 분석 #\n",
        "###############\n",
        "\n",
        "# nltk 모듈 활용\n",
        "# vocab().most_commmon (가장 자주 사용하는 빈도수)\n",
        "\n",
        "# Python 자료구조 4가지: \n",
        "#  - List, Dict, Tuple, Set\n",
        "#  - List: 순차적(인덱스) [1, 2, 3]\n",
        "#  - Dict: 비순차적(Key)  {\"number\": 1}\n",
        "#  - Set: 순서 없음, 중복 없음 : 집합\n",
        "#    → set() : ()안의 값을 set자료형으로 변환\n",
        "#    → set()을 사용하면 type set으로 변경\n",
        "#    ※ list(set(\"data\"))\n",
        "#       data → set Type → list Type\n",
        "text = nltk.Text(tokens, name='NSMC')\n",
        "print(f'전체 Token : {len(text.tokens)}')\n",
        "print(f'전체 Token(중복 제외) : {len(set(text.tokens))}')\n",
        "print(f'빈도수가 높은 상위 10개: {text.vocab().most_common(10)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZcFAitYTHRx",
        "outputId": "08f41d70-275d-4950-e6b2-b1f94618cca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 Token : 2159921\n",
            "전체 Token(중복 제외) : 49895\n",
            "빈도수가 높은 상위 10개: [('./Punctuation', 67778), ('영화/Noun', 50818), ('하다/Verb', 41209), ('이/Josa', 38540), ('보다/Verb', 38538), ('의/Josa', 30188), ('../Punctuation', 29055), ('가/Josa', 26627), ('에/Josa', 26468), ('을/Josa', 23118)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 높은 5000개의 토큰을 사용\n",
        "select_words = [f[0] for f in text.vocab().most_common(5000)]\n",
        "print(f'type: {type(select_words)}')\n",
        "print(f'len: {len(select_words)}')\n",
        "print(f'data: {select_words[:10]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYCOmK0wWkhw",
        "outputId": "29b10f3b-9a82-4d1d-f32c-a872e85fc7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: <class 'list'>\n",
            "len: 5000\n",
            "data: ['./Punctuation', '영화/Noun', '하다/Verb', '이/Josa', '보다/Verb', '의/Josa', '../Punctuation', '가/Josa', '에/Josa', '을/Josa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 학습 데이터 생성 #\n",
        "####################\n",
        "\n",
        "# select_words → 인공지능 학습용 데이터\n",
        "# 저장!\n",
        "if os.path.isfile('/content/drive/MyDrive/nsmc/selectword.txt') == False:\n",
        "  f = open('selectword.txt', 'w', encoding='UTF-8')\n",
        "  for i in select_words:\n",
        "    i += '\\n' # \\n 한줄개행\n",
        "    f.write(i)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "WQX5iv9VWysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰 → [토큰1, 토큰2, 토큰3, 토큰4, ...]\n",
        "#         5000개에 포함되는거 Count\n",
        "#         해당 리뷰에는 5000안에 포함 되는 토큰 이 몇개\n",
        "# 이진분류 → 데이터(리뷰 토큰), 정답(0, 1)\n",
        "# 데이터(x), 정답(y)\n",
        "\n",
        "def term_frequency(doc):\n",
        "  return [doc.count(word) for word in select_words]\n",
        "\n",
        "train_x = [term_frequency(d) for d, _ in train_docs]\n",
        "train_y = [c for _, c in train_docs]\n",
        "# [0, 1, 1, 0, 1, 0, ...]\n",
        "test_x = [term_frequency(d) for d, _ in test_docs] \n",
        "test_y = [c for _, c in test_docs]\n",
        "print(train_x)\n",
        "print(train_y)"
      ],
      "metadata": {
        "id": "Ys9hqGNAY3Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습데이터 분리 (x:리뷰, y:정답)\n",
        "# tensorflow + keras → float Type 형태로 변환\n",
        "#  - float Type 형변환\n",
        "\n",
        "x_train = np.asarray(train_x).astype('float32')\n",
        "y_train = np.asarray(train_y).astype('float32')\n",
        "x_test = np.asarray(test_x).astype('float32')\n",
        "y_test = np.asarray(test_y).astype('float32')"
      ],
      "metadata": {
        "id": "KmnGAXtYb19H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 딥러닝 모델 생성 #\n",
        "####################\n",
        "\n",
        "# 딥러닝 모\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(5000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "####################\n",
        "# 딥러닝 모델 설정 #\n",
        "####################\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])\n",
        "\n",
        "####################\n",
        "# 딥러닝 모델 학습 #\n",
        "####################\n",
        "model.fit(x_train, y_train, epoch=10, batch_size=512)\n",
        "\n",
        "####################\n",
        "# 딥러닝 모델 평가 #\n",
        "####################\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "####################\n",
        "# 딥러닝 모델 저장 #\n",
        "####################\n",
        "model.save('/content/drive/MyDrive/nsmc/nsmc_model.h5')"
      ],
      "metadata": {
        "id": "OtLhWAIadgxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 사용하기"
      ],
      "metadata": {
        "id": "1VFT5tf7DNX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 완성\n",
        "#  → my_model.h5\n",
        "#  → h5 : (모델의구조 + 학습가중치)\n",
        "#  → 단순하게 weight만 저장도 가능\n",
        "#     사용하실 때 모델 구조를 다시 생성(기억)\n",
        "\n",
        "# 학습한 모델 → 긍정, 부정 분류기(87%)\n",
        "# → 영화 리뷰를 입력으로 넣어서 결과를 확인(활용)\n",
        "reply_list = []              # MongoDB Document를 담을 List\n",
        "selected_words = []          # selectword.txt를 담을 List\n",
        "model = None                 # 트레이닝 된 모델을 담을 변수\n",
        "okt = Okt()                  # Okt() 형태소 분석기 객체 생성\n",
        "all_count = 0                # 전체갯수\n",
        "pos_count = 0                # 긍정갯수"
      ],
      "metadata": {
        "id": "xa0hMv4LDWh_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# 데이터베이스 연결 #\n",
        "#####################\n",
        "from pymongo import MongoClient\n",
        "client = MongoClient(\"mongodb+srv://cnu:cnu1234@cluster0.waenlcv.mongodb.net/?retryWrites=true&w=majority\")     # MongoDB 찾아감\n",
        "db = client['movie']                         # Database 선택\n",
        "collection = db.get_collection('review')     # Collection 선택"
      ],
      "metadata": {
        "id": "pcoOimO9FVz1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# 데이터베이스에서 데이터 가져오기 #\n",
        "####################################\n",
        "\n",
        "def get_reviews():\n",
        "  reply_list = []\n",
        "  for one in collection.find({}, {'_id':0, 'title':1, 'review':1, 'score':1}):\n",
        "    reply_list.append([one['title'], one['review'], one['score']])\n",
        "  return reply_list"
      ],
      "metadata": {
        "id": "yJ2dv0VxFs22"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply_list = get_reviews()\n",
        "all_count = len(reply_list)\n",
        "print(all_count)\n",
        "print(reply_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlRHBb_xHYHP",
        "outputId": "d25626eb-7cd7-44f7-fa0c-6d1614c2cb63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "766\n",
            "['폴: 600미터', '한정된 공간 안에서 스토리를 풀어나가기 쉽지 않은데 긴장감과 간절함 있게 잘 풀어나간 영화.', '8']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# 데이터베이스 전처리 #\n",
        "#######################\n",
        "# 인공지능 input에 맞게 전처리!\n",
        "def read_data(file_name):\n",
        "    words_data = []\n",
        "    with open(file_name, 'r', encoding='UTF8') as f:\n",
        "        while True:\n",
        "            line = f.readline()[:-1]\n",
        "            if not line: break\n",
        "            words_data.append(line)\n",
        "    return words_data\n",
        "\n",
        "selected_words = read_data('/content/drive/MyDrive/nsmc/selectword.txt')\n",
        "print(selected_words[:5])  "
      ],
      "metadata": {
        "id": "zW3w5HPnJdgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e73363c-208e-467f-c804-2575449e9326"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./Punctuation', '영화/Noun', '하다/Verb', '이/Josa', '보다/Verb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측할 데이터의 전처리를 진행할 메서드\n",
        "def tokenize(doc):\n",
        "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
        "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
        "\n",
        "# 예측할 데이터의 벡터화를 진행할 메서드(임베딩)\n",
        "def term_frequency(doc):\n",
        "    return [doc.count(word) for word in selected_words]"
      ],
      "metadata": {
        "id": "xAzem2fLs2WI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# 학습 완료 된 인공지능 모델 불러오기 #\n",
        "#######################################\n",
        "model = models.load_model('/content/drive/MyDrive/nsmc/my_model.h5')\n",
        "print(f'model(type): {type(model)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P2v5xmnsZW2",
        "outputId": "14118db9-766c-475f-c041-3a513b67b106"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model(type): <class 'keras.engine.sequential.Sequential'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델로 예측하는 메서드 구현\n",
        "def predict_pos_neg(review):\n",
        "    token = tokenize(review)\n",
        "    tf = term_frequency(token)\n",
        "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
        "    score = float(model.predict(data))\n",
        "    if (score > 0.5):\n",
        "        global pos_count\n",
        "        pos_count += 1\n",
        "        print(\"[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\\n\".format(review, score * 100))\n",
        "    else:\n",
        "        print(\"[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\\n\".format(review, (1 - score) * 100))"
      ],
      "metadata": {
        "id": "9LsdiYoes3fb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 예측시작\n",
        "def predict():\n",
        "    for one in reply_list[:10]:\n",
        "        predict_pos_neg(one[1])\n",
        "\n",
        "    aCount = all_count\n",
        "    pCount = pos_count\n",
        "    pos_pct = (pCount*100)/aCount\n",
        "    neg_pct = 100-pos_pct\n",
        "    # print(aCount, pCount, pos_pct, neg_pct)\n",
        "    print('▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒')\n",
        "    print('▒▒({}) 댓글 {}개를 감성분석한 결과'.format(reply_list[0][0], aCount))\n",
        "    print('▒▒긍정적인 의견{:.2f}% / 부정적인 의견{:.2f}% '.format(pos_pct, neg_pct))\n",
        "    print('▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒')\n",
        "\n",
        "predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-jvaIdQssQ_",
        "outputId": "bbe38276-8819-4454-a708-37fde3d9c129"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "[한정된 공간 안에서 스토리를 풀어나가기 쉽지 않은데 긴장감과 간절함 있게 잘 풀어나간 영화.]는 96.29% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[와 대박이네. 중간에 알포인트를 능가하는 소름이 ㄷㄷ]는 94.43% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[언더 워터 공중편, 뻔하지 않았고 스릴러로서 적당히 재밌었다.요즘 이렇게 적당히 재밌는 영화도 보기 힘들었었는데 대만족이었음]는 99.98% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[위험한 길로 이끄는 친구 조심~]는 58.78% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[와 몰입해서 땀 질질 흘리면서 봤음]는 95.46% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[와. . . 반전반전 대반전. . . 근데 좀 잔인한 장면들도잇네요. 그치만 너무 쫄깃합니다 상영시간 내내. . . 올 연말 강추영화]는 68.98% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[극장에서만 긴장감을 느낄 수 있는 영화 스토리도 나름 괜찮았다]는 99.29% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[따봉충들에게 고하는 메시지]는 54.96% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[그곳에 간 이유의 합당함 여부보다는 장르물로서 목표 달성을 했느냐가 중요한데 이 영화는 확실히 달성했습니다.]는 91.76% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[괜찮네요 저예산 영화 같은데 순간순간이 스릴있네요]는 98.81% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
            "\n",
            "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒\n",
            "▒▒(폴: 600미터) 댓글 766개를 감성분석한 결과\n",
            "▒▒긍정적인 의견24.93% / 부정적인 의견75.07% \n",
            "▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NagGrEYbC4pxrkvhvgRFpS8DLjOblXzD",
      "authorship_tag": "ABX9TyOh86+CltHSwZ0sOCI3et+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}